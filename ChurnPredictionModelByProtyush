#1
import pandas as pd

# Read CSV file
#file_path = ''  # Replace with your file path
df = pd.read_csv('CustomerChurn.csv')

# Check for missing or null values
missing_values = df.isnull().sum()

# Print the columns with missing values
print("Missing values in each column:")
print(missing_values)

# Print rows with missing values
rows_with_missing = df[df.isnull().any(axis=1)]
print("\nRows with missing values:")
print(rows_with_missing)

# Summary of missing values
total_missing = missing_values.sum()
print(f"\nTotal number of missing values in the dataset: {total_missing}")

if total_missing > 0:
    print("\nColumns with missing values:")
    for column, count in missing_values.items():
        if count > 0:
            print(f"{column}: {count} missing values")
else:
    print("\nNo missing values in the dataset.")





#2
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = 'CustomerChurn.csv'
df = pd.read_csv(file_path)

# Convert the 'Churn' column to numerical values
df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Ensure 'TotalCharges' is numeric (some values may be non-numeric)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Check for missing values and handle them (e.g., fill with mean or drop)
df['TotalCharges'].fillna(df['TotalCharges'].mean(), inplace=True)

# List of numerical features
numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Display basic information about the dataset
print(df.info())

# Display the first few rows of the dataset
print(df.head())

# Display summary statistics
print(df.describe())

# Visualizing distributions of numerical features

# Histograms
df[numerical_features].hist(bins=15, figsize=(15, 6), layout=(2, 3))
plt.suptitle('Histograms of Numerical Features')
plt.show()

# Boxplots
plt.figure(figsize=(15, 6))
for i, feature in enumerate(numerical_features, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(y=df[feature])
    plt.title(f'Boxplot of {feature}')
plt.tight_layout()
plt.show()


#3
# Correlation matrix
correlation_matrix = df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()


#4
# List of categorical features
categorical_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 
                        'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 
                        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 
                        'Contract', 'PaperlessBilling', 'PaymentMethod']

# Bar plots for categorical features
plt.figure(figsize=(20, 20))
for i, feature in enumerate(categorical_features, 1):
    plt.subplot(4, 4, i)
    sns.countplot(data=df, x=feature, hue='Churn')
    plt.title(f'Count Plot of {feature}')
    plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


#5
# Compare features between churned and non-churned customers

# Numerical features
plt.figure(figsize=(15, 6))
for i, feature in enumerate(numerical_features, 1):
    plt.subplot(1, 3, i)
    sns.boxplot(x='Churn', y=feature, data=df)
    plt.title(f'{feature} by Churn')
plt.tight_layout()
plt.show()

# Categorical features
plt.figure(figsize=(20, 20))
for i, feature in enumerate(categorical_features, 1):
    plt.subplot(4, 4, i)
    sns.countplot(data=df, x=feature, hue='Churn')
    plt.title(f'{feature} by Churn')
    plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

file_path = 'CustomerChurn.csv'
df = pd.read_csv(file_path)

# Convert the 'Churn' column to numerical values
df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Encode categorical features
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

X = df.drop(columns=['Churn'])
y = df['Churn']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print feature importances
print(feature_importances)


#For Random Forest
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

file_path = 'CustomerChurn.csv'
df = pd.read_csv(file_path)
df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

# Selected features based on importance
selected_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Contract', 'OnlineSecurity', 'PaymentMethod', 'TechSupport']

X = df[selected_features]
y = df['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

#For logistic Regression
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

file_path = 'CustomerChurn.csv'
df = pd.read_csv(file_path)

df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

selected_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'Contract', 'OnlineSecurity', 'PaymentMethod', 'TechSupport']

X = df[selected_features]
y = df['Churn']

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = [
    {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear'], 'max_iter': [100, 200, 300]},
    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs', 'saga'], 'max_iter': [100, 200, 300]},
    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['saga'], 'max_iter': [100, 200, 300], 'l1_ratio': [0, 0.5, 1]}
]

# Initialize the Logistic Regression Classifier
log_reg = LogisticRegression(random_state=42)

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Get the best estimator
best_log_reg = grid_search.best_estimator_

# Make predictions
y_pred = best_log_reg.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Output
print("Best Parameters:")
print(grid_search.best_params_)


